---
title: "ChatGPT"
layout: single
---

> ChatGPT 관련 자료 모음 입니다.

## 1. 어텐션과 트랜스포머
* [Part 1][1] : [SimpleAttention][1-1]
* [Part 2][2] : [Attn: Illustrated Attention][2-1]
* [Part 3][3] : [Bias Variance 개념정리][3-1], [Positional Encoding][3-2]
* [Part 4][4] : [Layer Normalization][4-1], [ResNet][4-2], [Tokenizer][4-3], [tiktoken][4-4], [OpenAI Tokenizer][4-5], [Multi-Head Attention][4-6]
## 2. 챗GPT API 활용
* [발표 자료][5]
* [실습-1][6] : [삼성전자 재무제표][6-1], [SK하이닉스 재무표][6-2]

[1]: https://youtu.be/Wp4hRuwiN3I
[1-1]: https://colab.research.google.com/drive/1yHXD_dkjsSSaRVCebTB5TNdeNFzIx-UO
[2]: https://youtu.be/x0mwvV1R4oQ
[2-1]: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#30c6
[3]: https://youtu.be/41KcONN3ok0
[3-1]: https://modulabs-biomedical.github.io/Bias_vs_Variance
[3-2]: https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model
[4]: https://youtu.be/4xF7D5Bo53c
[4-1]: https://becominghuman.ai/all-about-normalization-6ea79e70894b
[4-2]: https://bskyvision.com/644
[4-3]: https://blog.floydhub.com/tokenization-nlp/
[4-4]: https://github.com/openai/tiktoken
[4-5]: https://platform.openai.com/tokenizer
[4-6]: https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/
[5]: https://drive.google.com/file/d/10rE5PyVOMho6o7Ji9fPVy8ZahdzQPSlM/view
[6]: https://chat.openai.com/
[6-1]: https://finance.yahoo.com/quote/005930.KS/balance-sheet?p=005930.KS
[6-2]: https://finance.yahoo.com/quote/000660.KS/balance-sheet?p=000660.KS

