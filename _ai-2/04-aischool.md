---
title: "AI School"
layout: single
---

> 서울데이터과학연구회의 인공지능융합학교 5기 강의 내용입니다.

## 인공지능융합학교(5기)
1. 머신러닝과 인공지능 기초 ([영상][11-1]. [자료][11-2]) 
  * [CNN][1-1], [맥클럭-피츠 모델][1-2], [Perceptron][1-3], [다층 퍼셉트론][1-4], [XOR 문제][1-5]
  * [6][1-6], [7][1-7], [8][1-8], [9][1-9], [10][1-10]
2. 딥러닝과 자연어처리, word2vec ([영상][22-1]. [자료][22-2]) 
  * [Projector][2-1], [유클리디언 거리][2-2], [Word Embedding][2-3]
3. [RNN vs FNN][3]
  * [Skip-Gram 모델][3-1]. [소프트맥스][3-2], [자연어처리][3-3], [wevi][3-4], [LSTM/GRU][3-5], [LSTM][3-6]
  * [LSTM 주가예측 문제][3-7]
4. 기계번역, sequence-to-sequence ([영상][40-1], [자료][40-2])
  * [SGD][4-1], [한땀한땀 딥러닝][4-2], [감정분석][4-3], [Seq-to-Seq][4-4], [Attention][4-5]
  * [감정분석 LSTM 문제][4-6]
5. [Transformer]
 
6. [GPT와 BERT]

## 인공지능융합학교(4기)
1. [기상 빅데이터터 활용][44-1]
2. [자연어 처리][44-2]
3. [인공지능 비즈니스 모델][44-3]

[11-1]: https://youtu.be/FvjzPPx5qJQ
[11-2]: https://drive.google.com/file/d/17Io8Rfu_ZpAqE86tvJf4HnZ62uY5i0fZ/view
[1-1]: https://drive.google.com/file/d/18G_SlLZI7k5TNvLYT-aWG5qkOgV8MNrH/view?usp=drive_link
[1-2]: https://www.geeksforgeeks.org/implementing-models-of-artificial-neural-network/
[1-3]: https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a
[1-4]: https://data-miner-gon.tistory.com/35
[1-5]: https://ang-love-chang.tistory.com/26
[1-6]: https://blog.naver.com/samsjang/220959562205
[1-7]: https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/
[1-8]: https://machinelearningmastery.com/the-chain-rule-of-calculus-for-univariate-and-multivariate-functions/
[1-9]: https://www.youtube.com/watch?v=1Q_etC_GHHk
[1-10]: https://www.youtube.com/watch?v=aircAruvnKk&t=760s
[22-1]: https://youtu.be/ocK7s0smPE4
[22-2]: https://drive.google.com/file/d/17KNZwCsGHFNE-Oo-g4lVrphEOf18O4ke/view
[2-1]: https://projector.tensorflow.org/
[2-2]: https://medium.com/@sasi24/cosine-similarity-vs-euclidean-distance-e5d9a9375fc8
[2-3]: http://ronxin.github.io/wevi/
[3]: https://drive.google.com/file/d/1OHAJ9YDez0dHTuLjz4cJSujckpxBs0LD/view?usp=drive_link
[3]: https://drive.google.com/file/d/1SKUEehNAv4lROIzTjLlSlQhoBj6oGRyD/view
[3-1]: https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling
[3-2]: https://www.youtube.com/watch?v=K7HTd_Zgr3w&t=1s
[3-3]: https://wikidocs.net/35476
[3-4]: http://ronxin.github.io/wevi/
[3-5]: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
[3-6]: https://www.youtube.com/watch?v=YCzL96nL7j0&t=696s
[3-7]: https://colab.research.google.com/drive/1SM6jPefAPgWqkuFj8xbu74TAVt3viwOl
[40-1]: https://youtu.be/gymMaJYEb18
[40-2]: https://drive.google.com/file/d/1SuxgGTd3ktiiqHvNV-AXFUruFcUKMusP/view
[4-1]: https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1
[4-2]: https://wikidocs.net/152765
[4-3]: https://happy-jihye.github.io/nlp/nlp-1/
[4-4]: https://arxiv.org/pdf/1409.3215.pdf
[4-5]: https://www.davidsbatista.net/blog/2020/01/25/Attention-seq2seq/
[4-6]: https://colab.research.google.com/drive/1Sx6iSrqs9AaBem7ND4d2kfL8MoYo-Izc
[50-1]: https://www.youtube.com/watch?v=v6FXxbMysuk
[50-2]: https://drive.google.com/file/d/1TwMc-K0syxK7xDZH_BqIKwvzPc_4qWJF/view

[44-1]: https://drive.google.com/file/d/1T-MFl4r48Jw1g6a1QwSYqpvA6I6X84Oy/view
[44-2]: https://colab.research.google.com/drive/1Sm8ZbmGsGJ_v1q9TCTtKZYXSswYAhIck
[44-3]: https://drive.google.com/file/d/1SZ6pa_XwQEwruh19Knkixb5HYFBDsRaO/view
